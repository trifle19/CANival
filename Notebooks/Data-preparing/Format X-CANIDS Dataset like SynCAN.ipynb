{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Formatting X-CANIDS Dataset like SynCAN\n",
    "\n",
    "https://dx.doi.org/10.21227/epsj-y384\n",
    "\n",
    "1. Download Raw CAN messages (raw.zip) from above URL\n",
    "2. Unzip raw.zip and place the parquet files at `Dataset/X-CANIDS/raw` directory.\n",
    "3. Run this code. It converts the X-CANIDS data files into the same format of SynCAN dataset. In the end of the code, it will save the formatted result files at `Dataset/X-CANIDS/canet` directory. It will also save signal-extracted intermediate files at `Dataset/X-CANIDS/signal` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Libraries and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T00:54:15.974363Z",
     "start_time": "2024-09-17T00:54:15.581375Z"
    }
   },
   "source": [
    "import gc\n",
    "import glob\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "import cantools\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option('display.float_format', str)\n",
    "dataset_dir = '../../Dataset/X-CANIDS'\n",
    "dbc = cantools.database.load_file(f'{dataset_dir}/hyundai_2015_ccan.dbc')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Key functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T00:54:16.061429Z",
     "start_time": "2024-09-17T00:54:16.056882Z"
    }
   },
   "source": [
    "def bytes_to_list(data_bytes: bytes) -> list:\n",
    "    l = len(data_bytes)\n",
    "    decimal_values = struct.unpack(f'{l}B', data_bytes)\n",
    "    return list(decimal_values)\n",
    "\n",
    "def load_arrange_data(file_path, print_option=True):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # make Session labels (Note: Attacks in X-CANIDS Dataset were performed without a pause)\n",
    "    df['Session'] = 0\n",
    "    splits = Path(file_path).stem.split('-')\n",
    "    attack = None\n",
    "    if len(splits) > 1:  # if it's an attack dataset\n",
    "        attack = splits[1]\n",
    "        msgs = df.loc[df['label'] == 1]\n",
    "        t_start, t_end = 0, 0\n",
    "        if msgs.shape[0] > 0:  # if the dataset includes attack messages\n",
    "            t_start, t_end = msgs.index.min(), msgs.index.max()\n",
    "            df.loc[t_start:t_end, 'Session'] = 1\n",
    "            assert df.query('label == 1 and Session == 0').shape[0] == 0   \n",
    "        if attack == 'susp':  # it doens't have label=1 rows, so apply a rougher approach\n",
    "            assert len(df.loc[(480 < df.index.total_seconds()) & (df.index.total_seconds() <= 1440) & (df['label'] == 1)]) == 0\n",
    "            df.loc[(480 < df.index.total_seconds()) & (df.index.total_seconds() <= 1440), 'Session'] = 1\n",
    "    # Format columns\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Time'] = df['timestamp'].dt.total_seconds()\n",
    "    df.rename(columns={'arbitration_id': 'ID', 'dlc': 'DLC', 'label': 'Label', 'data': 'Data'}, inplace=True)\n",
    "    df['Session'] = df['Session'].astype(int)\n",
    "    df['Label'] = df['Label'].astype(int)\n",
    "    if print_option:\n",
    "        print(f'# rows: {df.shape[0]:,}')\n",
    "        print(pd.concat([df['Label'].value_counts().rename('Label'), df['Session'].value_counts().rename('Session')], axis=1))\n",
    "    return df[['Session', 'Label', 'Time', 'ID', 'DLC', 'Data']]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T00:54:16.755721Z",
     "start_time": "2024-09-17T00:54:16.750920Z"
    }
   },
   "source": [
    "def iterchunk(dataset: pd.DataFrame, n: int):\n",
    "    prev = 0\n",
    "    while True:\n",
    "        chunk = dataset.iloc[prev:prev+n].copy()\n",
    "        if chunk.shape[0] == 0:\n",
    "            break\n",
    "        prev += n\n",
    "        yield chunk\n",
    "\n",
    "def decode(record) -> dict:\n",
    "    try:\n",
    "        message = dbc.get_message_by_frame_id(record['ID'])\n",
    "        decoded = message.decode(record['Data'], decode_choices=False, allow_truncated=False, allow_excess=False)\n",
    "        '''\n",
    "        decode_choices=False does not convert the decoded values to choice strings\n",
    "        allow_truncated=False and allow_excess=False do not accept a longer or shorter Data field than specified\n",
    "        '''\n",
    "        decoded_v2 = {}\n",
    "        for key, value in decoded.items():\n",
    "            decoded_v2[f\"{str(record['ID'])}+{key}\"] = value\n",
    "        return decoded_v2\n",
    "    except KeyError:\n",
    "        return {}\n",
    "\n",
    "def parse_with_DBC(data: pd.DataFrame, label=True) -> pd.DataFrame:\n",
    "    data['decoded'] = data.apply(decode, axis=1)\n",
    "    parsed_data = pd.DataFrame.from_records(data.decoded.reset_index(drop=True))\n",
    "    columns = parsed_data.columns.to_list()\n",
    "    if label:\n",
    "        default_columns = ['Session', 'Label', 'Time']\n",
    "    else:\n",
    "        default_columns = ['Time']\n",
    "    parsed_data[default_columns] = data[default_columns].to_numpy()     # to_numpy() is applied to bypass index alignment\n",
    "    parsed_data = parsed_data[default_columns + columns]    # rearrange the column order (monotime and labels to the first)\n",
    "    return parsed_data\n",
    "\n",
    "# def select_meaningful_columns(dataset: pd.DataFrame, chunk_size: int) -> list:\n",
    "#     n_chunk = dataset.shape[0] // chunk_size + 1\n",
    "#     minmax_list = list()\n",
    "#     for chunk in tqdm(iterchunk(dataset, n=chunk_size), desc=' - Checking min/max values of all signals', total=n_chunk):\n",
    "#         parsed_data = parse_with_DBC(chunk)\n",
    "#         minmax = pd.concat([parsed_data.min(numeric_only=True).rename('min'),\n",
    "#                             parsed_data.max(numeric_only=True).rename('max')],\n",
    "#                            axis=1)\n",
    "#         minmax_list.append(minmax)\n",
    "#     final_minmax = pd.concat(minmax_list, join='outer', axis=1)\n",
    "#     final_minmax = pd.concat([final_minmax.min(axis=1).rename('min'), final_minmax.max(axis=1).rename('max')], axis=1)\n",
    "#     final_minmax['constant'] = 0\n",
    "#     final_minmax.loc[final_minmax['min'] == final_minmax['max'], 'constant'] = 1\n",
    "#     columns = final_minmax.query('constant == 0').index.to_list()\n",
    "#     print(f' - {len(columns)} columns are valid.')\n",
    "#     return columns"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parse selected signals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T00:54:18.070818Z",
     "start_time": "2024-09-17T00:54:18.067403Z"
    }
   },
   "source": [
    "def parse_selected_signals(dataset: pd.DataFrame, chunk_size: int, signals: list) -> pd.DataFrame:\n",
    "    n_chunk = dataset.shape[0] // chunk_size + 1\n",
    "    default_columns = ['Session', 'Label', 'Time']\n",
    "    data = pd.DataFrame(columns=default_columns + signals)    # Define an empty DataFrame with signal names\n",
    "    for chunk in tqdm(iterchunk(dataset, n=chunk_size), desc=' - Parsing signals', total=n_chunk):\n",
    "        parsed_data = parse_with_DBC(chunk)\n",
    "        subset = default_columns + list(set(signals) & set(parsed_data.columns))\n",
    "        data = pd.concat([data, parsed_data[subset]], join='outer', axis=0, ignore_index=True)\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T00:58:17.881098Z",
     "start_time": "2024-09-17T00:58:17.876299Z"
    }
   },
   "source": [
    "selected_signals, continuous_signals = [], []\n",
    "with open(f'{dataset_dir}/canet_signals_continuous.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        continuous_signals.append(line.strip())\n",
    "selected_signals += continuous_signals\n",
    "with open(f'{dataset_dir}/canet_signals_categorical.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        selected_signals.append(line.strip())\n",
    "\n",
    "print(f'# selected signals: {len(selected_signals)}')\n",
    "ids = set([int(x.split('+')[0]) for x in selected_signals])\n",
    "ids_hex = [f'{hex(x)[2:].upper().zfill(3)}h' for x in ids]\n",
    "ids_hex.sort()\n",
    "print(f'# CAN IDs: {len(ids)}')\n",
    "print(f'CAN IDs: {ids}')\n",
    "print(f'CAN IDs in hex: {ids_hex}')\n",
    "\n",
    "can_datasets = glob.glob(f'{dataset_dir}/raw/dump*.parquet')\n",
    "can_datasets.sort()\n",
    "print(f'Raw data files ({len(can_datasets)}):')\n",
    "for f in can_datasets[:3] + ['...'] + can_datasets[-3:]:\n",
    "    print(f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# selected signals: 107\n",
      "# CAN IDs: 35\n",
      "CAN IDs: {128, 129, 512, 897, 899, 902, 903, 1419, 1292, 399, 273, 274, 1427, 275, 790, 1440, 544, 809, 1322, 1456, 688, 1345, 68, 1349, 1351, 1353, 593, 1363, 1365, 1366, 1367, 608, 354, 1265, 1151}\n",
      "CAN IDs in hex: ['044h', '080h', '081h', '111h', '112h', '113h', '162h', '18Fh', '200h', '220h', '251h', '260h', '2B0h', '316h', '329h', '381h', '383h', '386h', '387h', '47Fh', '4F1h', '50Ch', '52Ah', '541h', '545h', '547h', '549h', '553h', '555h', '556h', '557h', '58Bh', '593h', '5A0h', '5B0h']\n",
      "Raw data files (133):\n",
      "../../Dataset/X-CANIDS/raw/dump1.parquet\n",
      "../../Dataset/X-CANIDS/raw/dump2.parquet\n",
      "../../Dataset/X-CANIDS/raw/dump3.parquet\n",
      "...\n",
      "../../Dataset/X-CANIDS/raw/dump6-susp-5B0h.parquet\n",
      "../../Dataset/X-CANIDS/raw/dump6.parquet\n",
      "../../Dataset/X-CANIDS/raw/dump7.parquet\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T01:23:59.222221Z",
     "start_time": "2024-09-17T01:13:24.062297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = 500000  ####### REDUCE THIS VALUE if you get a memory error #######\n",
    "\n",
    "print(f'### Extracting CAN features ###')\n",
    "for i, p in enumerate(can_datasets):\n",
    "    print(f' [{i+1}/{len(can_datasets)}] Processing dataset: {p}')\n",
    "    savepath = Path(f\"{dataset_dir}/signal/{Path(p).name}\")\n",
    "    if savepath.exists():\n",
    "        print(' - Skip extracting because the output already exists.')\n",
    "        continue\n",
    "    else:\n",
    "        data = load_arrange_data(p, print_option=False)\n",
    "        features = parse_selected_signals(data, chunk_size=chunk_size, signals=selected_signals)\n",
    "        features.to_parquet(savepath, engine='pyarrow', compression='snappy')\n",
    "        print(f' - Saved: {savepath}')\n",
    "        del features\n",
    "        gc.collect()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Extracting CAN features ###\n",
      " [1/133] Processing dataset: ../../Dataset/X-CANIDS/raw/dump1.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " - Parsing signals:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa18420eaeb4067a787497209bab859"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/0f0_bgm56jn9r9w9_7hx5rb80000gn/T/ipykernel_10381/3911200721.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, parsed_data[subset]], join='outer', axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Saved: ../../Dataset/X-CANIDS/signal/dump1.parquet\n",
      " [2/133] Processing dataset: ../../Dataset/X-CANIDS/raw/dump2.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " - Parsing signals:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd08fe0c2934c58bb87c6459698689f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/0f0_bgm56jn9r9w9_7hx5rb80000gn/T/ipykernel_10381/3911200721.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, parsed_data[subset]], join='outer', axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Saved: ../../Dataset/X-CANIDS/signal/dump2.parquet\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scaling & fomatting like a SynCAN data file\n",
    "Session,    Label,  Time,           ID,     Signal1_of_ID,      Signal2_of_ID,  Signal3_of_ID,  Signal4_of_ID, ...\n",
    "\n",
    "Normal,     Normal, 63006572.0314,  id1,    0.3365969165117973, 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Check and revise DBC min & max values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T01:36:22.322650Z",
     "start_time": "2024-09-17T01:35:42.091860Z"
    }
   },
   "source": [
    "# Get signal dictionary from DBC\n",
    "signal_info = dict()\n",
    "for message in dbc.messages:\n",
    "    for signal in message.signals:\n",
    "        signal_info[f'{message.frame_id}+{signal.name}'] = {'scale': signal.scale, 'min': signal.minimum, 'max': signal.maximum}\n",
    "\n",
    "# Distribution of selected signals in normal conditions\n",
    "sig_datasets = glob.glob(f'{dataset_dir}/signal/dump[1-9].parquet')\n",
    "for dataset in sig_datasets:\n",
    "    print(f'Checking {dataset}')\n",
    "    df = pd.read_parquet(dataset, columns=['Session', 'Label', 'Time'] + selected_signals)\n",
    "    df_describe = df.describe()\n",
    "    # display(df_describe)\n",
    "\n",
    "    # Compare DBC specifications and real data\n",
    "    print('**** Wrong specifications in DBC ****')\n",
    "    for sig, info in signal_info.items():\n",
    "        try:\n",
    "            real_min, real_max = round(df_describe.loc['min', sig], 6), round(df_describe.loc['max', sig], 6)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if info['min'] > real_min:\n",
    "            print(f'{[sig]} DBC min = {info[\"min\"]}, Real min = {real_min}')\n",
    "        if info['max'] < real_max:\n",
    "            print(f'{[sig]} DBC max = {info[\"max\"]}, Real max = {real_max}')\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ../../Dataset/X-CANIDS/signal/dump7.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 24.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 935.5828\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump6.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 105.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2463.9618\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump4.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 126.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2422.9446\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump5.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 90.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2391.6934\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump1.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 86.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2298.9164\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump3.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 93.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2639.7498\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump2.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "['354+Clutch_Driving_Tq'] DBC min = 0.0, Real min = -20.0\n",
      "['354+Clutch_Driving_Tq'] DBC max = 0.0, Real max = 125.0\n",
      "['354+Cluster_Engine_RPM'] DBC max = 0.0, Real max = 2516.6982\n",
      "['354+Cluster_Engine_RPM_Flag'] DBC max = 0.0, Real max = 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T01:36:40.416796Z",
     "start_time": "2024-09-17T01:36:40.414354Z"
    }
   },
   "source": [
    "# Revise few wrong value ranges heuristically\n",
    "signal_info['354+Clutch_Driving_Tq']['min'], signal_info['354+Clutch_Driving_Tq']['max'] = -512, 511\n",
    "signal_info['354+Cluster_Engine_RPM']['min'], signal_info['354+Cluster_Engine_RPM']['max'] = 0, 8191\n",
    "signal_info['354+Cluster_Engine_RPM_Flag']['min'], signal_info['354+Cluster_Engine_RPM_Flag']['max'] = 0, 1"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T01:38:08.565651Z",
     "start_time": "2024-09-17T01:37:27.653079Z"
    }
   },
   "source": [
    "# Check it again - if there's nothing stated under \"Wrong specifications\", you're good to go.\n",
    "for dataset in sig_datasets:\n",
    "    print(f'Checking {dataset}')\n",
    "    df = pd.read_parquet(dataset, columns=['Session', 'Label', 'Time'] + selected_signals)\n",
    "    df_describe = df.describe()\n",
    "    print('**** Wrong specifications in DBC ****')\n",
    "    for sig, info in signal_info.items():\n",
    "        try:\n",
    "            real_min, real_max = round(df_describe.loc['min', sig], 6), round(df_describe.loc['max', sig], 6)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if info['min'] > real_min:\n",
    "            print(f'{[sig]} DBC min = {info[\"min\"]}, Real min = {real_min}')\n",
    "        if info['max'] < real_max:\n",
    "            print(f'{[sig]} DBC max = {info[\"max\"]}, Real max = {real_max}')\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ../../Dataset/X-CANIDS/signal/dump7.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump6.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump4.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump5.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump1.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump3.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n",
      "Checking ../../Dataset/X-CANIDS/signal/dump2.parquet\n",
      "**** Wrong specifications in DBC ****\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Min-max normalization (based on DBC) and format like SynCAN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T01:41:12.116616Z",
     "start_time": "2024-09-17T01:40:59.639628Z"
    }
   },
   "source": [
    "sig_datasets = glob.glob(f'{dataset_dir}/signal/dump*.parquet')\n",
    "\n",
    "for dataset in tqdm(sig_datasets, desc=f'Processing {len(selected_signals)}-signal version'):\n",
    "    df = pd.read_parquet(dataset, columns=['Session', 'Label', 'Time'] + selected_signals)\n",
    "\n",
    "    # Min-max normalization\n",
    "    for signal in selected_signals:\n",
    "        min, max = signal_info[signal]['min'], signal_info[signal]['max']\n",
    "        df[signal] = ((df[signal] - min) / (max - min)).round(8)\n",
    "\n",
    "    # Formatting\n",
    "    df = df.reset_index().rename(columns={'index': 'MsgIndex'})\n",
    "    df_ids = []\n",
    "    for id in ids:\n",
    "        df_id = df.filter(like=f'{id}+', axis=1).dropna(how='all', axis=0)\n",
    "        signals = list(df_id.columns)\n",
    "        df_id = pd.concat([df.loc[:, ['MsgIndex', 'Session', 'Label', 'Time']], df_id], join='inner', axis=1)\n",
    "        df_id['ID'] = id\n",
    "        for i, signal in enumerate(signals):\n",
    "            df_id[f'Signal{i+1}'] = df_id.loc[:, signal]\n",
    "        df_id.drop(columns=signals, inplace=True)\n",
    "        df_ids.append(df_id)\n",
    "    df_canet = pd.concat(df_ids, axis=0).sort_values('MsgIndex', ignore_index=True)\n",
    "    df_canet = df_canet.astype({\"Session\": 'int', \"Label\": 'int'})\n",
    "    # display(df_canet.head(1))\n",
    "    assert df_canet['Signal1'].isna().sum() == 0, 'Some values in Signal1 is empty.'\n",
    "    filename = Path(dataset).name\n",
    "    save_path = f'{dataset_dir}/canet/sig{len(selected_signals):03}_{filename}'\n",
    "    df_canet.to_parquet(save_path)\n",
    "    print(f' - Saved: {savepath}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing 107-signal version:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa35fbeff47c4ca49120c2c6fc9a419b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Saved: ../../Dataset/X-CANIDS/signal/dump2.parquet\n",
      " - Saved: ../../Dataset/X-CANIDS/signal/dump2.parquet\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
