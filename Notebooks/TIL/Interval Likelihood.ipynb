{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T06:50:30.354057Z",
     "start_time": "2024-09-13T06:50:29.560971Z"
    }
   },
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import struct\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, rv_histogram\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "from pympler import asizeof\n",
    "\n",
    "pd.set_option('display.float_format', str)\n",
    "\n",
    "######### SELECT THE DATASET #########\n",
    "# DATASET = 'Syncan'\n",
    "DATASET = 'X-CANIDS'\n",
    "DATASET_DIR = f'../../Dataset/{DATASET}'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-09-13T06:50:36.901058Z",
     "start_time": "2024-09-13T06:50:36.894557Z"
    }
   },
   "source": [
    "def str_to_list(data_str: str) -> list:\n",
    "    data_list_str = data_str.split()\n",
    "    data_list = [int(x) for x in data_list_str]\n",
    "    if len(data_list) < 8:  # fill with dummy values (0) to 8 bytes\n",
    "        data_list += [0] * (8 - len(data_list))\n",
    "    return data_list\n",
    "\n",
    "def bytes_to_list(data_bytes: bytes) -> list:\n",
    "    l = len(data_bytes)\n",
    "    decimal_values = struct.unpack(f'{l}B', data_bytes)\n",
    "    return list(decimal_values)\n",
    "\n",
    "def load_arrange_data(file_path, dataset, print_option=True):\n",
    "    if dataset == 'Syncan':\n",
    "        df = pd.read_csv(file_path, delimiter=',')\n",
    "        df['Time'] = round(df['Time'] / 1000, 7)     # milliseconds to seconds\n",
    "        df.rename(columns={'Label': 'Session'}, inplace=True)\n",
    "        df['SessionCat'] = 'Normal'\n",
    "        df.loc[df['Session'] == 1, 'SessionCat'] = 'Attack'\n",
    "        if print_option:\n",
    "            print(f'# rows: {df.shape[0]:,}')\n",
    "            print(df['Session'].value_counts())\n",
    "        return df\n",
    "    elif dataset == 'X-CANIDS':\n",
    "        df = pd.read_parquet(file_path)\n",
    "        # make SessionCat\n",
    "        df['Label'] = 'Normal'\n",
    "        splits = Path(file_path).stem.split('-')\n",
    "        attack_map_dict = {'fabr': 'Fabrication', 'fuzz': 'Fuzzing', 'masq': 'Masquerade', 'repl': 'Replay', 'susp': 'Suspension'}\n",
    "        attack = None\n",
    "        if len(splits) > 1:\n",
    "            attack, aidh = attack_map_dict[splits[1]], splits[2]\n",
    "            df.loc[df['label'] == 1, 'Label'] = attack\n",
    "        # Make Session labels (Note: Attacks in X-CANIDS Dataset were performed without a pause)\n",
    "        df['Session'] = 0\n",
    "        df['SessionCat'] = 'Normal'\n",
    "        msgs = df.loc[df['label'] == 1]\n",
    "        t_start, t_end = 0, 0\n",
    "        if msgs.shape[0] > 0:  # if the dataset includes attack messages\n",
    "            t_start, t_end = msgs.index.min(), msgs.index.max()\n",
    "            df.loc[t_start:t_end, 'Session'] = 1\n",
    "            df.loc[t_start:t_end, 'SessionCat'] = attack\n",
    "            assert df.query('label == 1 and Session == 0').shape[0] == 0   \n",
    "        if attack == 'Suspension':  # it doens't have label=1 rows, so apply a rough approach\n",
    "            assert len(df.loc[(480 < df.index.total_seconds()) & (df.index.total_seconds() <= 1440) & (df['label'] == 1)]) == 0\n",
    "            df.loc[(480 < df.index.total_seconds()) & (df.index.total_seconds() <= 1440), 'Session'] = 1\n",
    "            df.loc[(480 < df.index.total_seconds()) & (df.index.total_seconds() <= 1440), 'SessionCat'] = attack\n",
    "        # Format columns\n",
    "        df.reset_index(inplace=True)\n",
    "        df['Time'] = df['timestamp'].dt.total_seconds()\n",
    "        df['Data'] = df['data'].apply(bytes_to_list)\n",
    "        df.rename(columns={'arbitration_id': 'ID', 'dlc': 'DLC'}, inplace=True)\n",
    "        if print_option:\n",
    "            print(f'# rows: {df.shape[0]:,}')\n",
    "            print(pd.concat([df['Label'].value_counts().rename('Label'), df['SessionCat'].value_counts().rename('SessionCat')], axis=1))\n",
    "        return df[['Session', 'SessionCat', 'Label', 'Time', 'ID', 'DLC', 'Data']]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define a training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:06:52.199999Z",
     "start_time": "2024-05-09T01:06:52.195151Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if DATASET == 'Syncan':\n",
    "    data_files = {\n",
    "        'train': [\n",
    "            f'{DATASET_DIR}/train_train.csv'\n",
    "        ],\n",
    "        'valid': [\n",
    "            f'{DATASET_DIR}/train_valid.csv'\n",
    "        ],\n",
    "        'test': [\n",
    "            f'{DATASET_DIR}/test_normal.csv',\n",
    "            f'{DATASET_DIR}/test_flooding.csv',\n",
    "            f'{DATASET_DIR}/test_plateau.csv',\n",
    "            f'{DATASET_DIR}/test_continuous.csv',\n",
    "            f'{DATASET_DIR}/test_playback.csv',\n",
    "            f'{DATASET_DIR}/test_suppress.csv'\n",
    "        ]\n",
    "    }\n",
    "elif DATASET == 'X-CANIDS':\n",
    "    data_files = {\n",
    "        'train': glob.glob(f'{DATASET_DIR}/raw/dump[1-4].parquet'),\n",
    "        'valid': [f'{DATASET_DIR}/raw/dump5.parquet'],\n",
    "        'test': glob.glob(f'{DATASET_DIR}/raw/dump6-*.parquet')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "# Statistical Feature Extraction"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:16:44.047195Z",
     "start_time": "2024-05-09T01:16:44.041762Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def time_cut(data: pd.DataFrame):\n",
    "    time_cut = data.loc[data.groupby('ID').cumcount() == 1, 'Time'].max()  # the timestamp that every ID has been occurred at least twice\n",
    "    return time_cut\n",
    "\n",
    "def get_counts(data: pd.DataFrame, cut: bool, ids=[], sessioncat=False):\n",
    "    if sessioncat:\n",
    "        base_cols = ['Session', 'SessionCat', 'Time', 'ID']\n",
    "    else:\n",
    "        base_cols = ['Session', 'Time', 'ID']\n",
    "    data = data[base_cols].copy()\n",
    "    n_rows = data.shape[0]\n",
    "    data['Timedelta'] = pd.to_timedelta(data['Time'], unit='s')\n",
    "    data_cnt = data[['ID', 'Timedelta']].copy()\n",
    "    data_cnt['ID'] = data_cnt['ID'].astype('category')\n",
    "    data_cnt = pd.get_dummies(data_cnt, prefix='', prefix_sep='').rolling('1s', on='Timedelta').sum()\n",
    "    id_cols = list(data_cnt.columns[1:])\n",
    "    if DATASET == 'X-CANIDS':\n",
    "        id_cols = [int(id_str) for id_str in id_cols]\n",
    "        data_cnt.columns = list(data_cnt.columns[:1]) + id_cols\n",
    "    data_cnt[id_cols] = data_cnt[id_cols].astype(int)\n",
    "    if ids:\n",
    "        data_cnt = data_cnt[list(data_cnt.columns[:1]) + ids]\n",
    "    base_cols.remove('ID')\n",
    "    data_cnt = pd.concat([data[base_cols + ['Timedelta']], data_cnt], axis=1).drop(columns=['Timedelta'])\n",
    "    if cut:\n",
    "        t_cut = time_cut(data)\n",
    "        data_cnt = data_cnt.loc[data_cnt['Time'] >= t_cut]\n",
    "        print(f'Data has been truncated to remove NaN ({n_rows - data_cnt.shape[0]:,} rows removed)')\n",
    "    assert data_cnt.isna().sum().sum() == 0\n",
    "    return data_cnt\n",
    "\n",
    "def get_time_intervals(data: pd.DataFrame, ffill: bool, cut: bool):\n",
    "    data = data[['Session', 'Time', 'ID']].copy()\n",
    "    n_rows = data.shape[0]\n",
    "    data['Interval'] = data.groupby('ID')['Time'].diff()\n",
    "    data_itv = pd.concat([data[['Session', 'Time']], data.pivot(columns='ID', values='Interval')], axis=1)\n",
    "    if ffill:\n",
    "        data_itv.ffill(inplace=True)\n",
    "    if cut:\n",
    "        t_cut = time_cut(data)\n",
    "        data_itv = data_itv.loc[data_itv['Time'] >= t_cut]\n",
    "        print(f'Data has been truncated to remove NaN ({n_rows - data_itv.shape[0]:,} rows removed)')\n",
    "    if ffill and cut:\n",
    "        assert data_itv.isna().sum().sum() == 0\n",
    "    return data_itv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:18:39.524958Z",
     "start_time": "2024-05-09T01:18:38.905202Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Dataset/X-CANIDS/raw/dump3.parquet\n",
      "# rows: 3,233,753\n",
      "          Label  SessionCat\n",
      "Normal  3233753     3233753\n",
      "# unique CAN IDs = 62\n",
      "Data has been truncated to remove NaN (5,960 rows removed)\n",
      "Data has been truncated to remove NaN (5,960 rows removed)\n",
      "../../Dataset/X-CANIDS/raw/dump4.parquet\n",
      "# rows: 4,761,327\n",
      "          Label  SessionCat\n",
      "Normal  4761327     4761327\n",
      "# unique CAN IDs = 64\n",
      "Data has been truncated to remove NaN (7,268 rows removed)\n",
      "Data has been truncated to remove NaN (7,268 rows removed)\n",
      "../../Dataset/X-CANIDS/raw/dump2.parquet\n",
      "# rows: 4,134,502\n",
      "          Label  SessionCat\n",
      "Normal  4134502     4134502\n",
      "# unique CAN IDs = 64\n",
      "Data has been truncated to remove NaN (58,764 rows removed)\n",
      "Data has been truncated to remove NaN (58,764 rows removed)\n",
      "../../Dataset/X-CANIDS/raw/dump1.parquet\n",
      "# rows: 3,123,785\n",
      "          Label  SessionCat\n",
      "Normal  3123785     3123785\n",
      "# unique CAN IDs = 62\n",
      "Data has been truncated to remove NaN (6,805 rows removed)\n",
      "Data has been truncated to remove NaN (6,805 rows removed)\n",
      "\n",
      "All ID-specific time intervals in the train dataset are concatenated.\n",
      "(15174570, 66)\n",
      "(15174570, 66)\n"
     ]
    }
   ],
   "source": [
    "ids_union = set()\n",
    "ids_inter = set()\n",
    "cnt_dfs = list()\n",
    "itv_dfs = list()\n",
    "\n",
    "total_rows = 0\n",
    "for data_file in data_files['train']:\n",
    "    print(f'{data_file}')\n",
    "    df_data = load_arrange_data(data_file, dataset=DATASET)\n",
    "    total_rows += df_data.shape[0]\n",
    "    unique_ids = set(df_data['ID'].unique())\n",
    "    print(f'# unique CAN IDs = {len(unique_ids)}')\n",
    "    ids_union |= unique_ids\n",
    "    if not ids_inter:\n",
    "        ids_inter = unique_ids.copy()\n",
    "    else:\n",
    "        ids_inter &= unique_ids\n",
    "    df_data_cnt = get_counts(df_data, cut=True)\n",
    "    df_data_itv = get_time_intervals(df_data, ffill=False, cut=True)\n",
    "    cnt_dfs.append(df_data_cnt)\n",
    "    itv_dfs.append(df_data_itv)\n",
    "\n",
    "df_cnt_concat = pd.concat(cnt_dfs, axis=0, ignore_index=True)\n",
    "df_itv_concat = pd.concat(itv_dfs, axis=0, ignore_index=True)\n",
    "print('\\nAll ID-specific time intervals in the train dataset are concatenated.')\n",
    "print(df_cnt_concat.shape)\n",
    "print(df_itv_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:18:54.954952Z",
     "start_time": "2024-05-09T01:18:54.951118Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAN IDs in every dataset (62): [66, 67, 68, 127, 128, 129, 273, 274, 275, 339, 354, 356, 399, 512, 544, 593, 608, 688, 790, 809, 897, 899, 902, 903, 1040, 1078, 1151, 1168, 1170, 1265, 1280, 1282, 1287, 1292, 1312, 1314, 1322, 1331, 1332, 1333, 1345, 1348, 1349, 1351, 1353, 1356, 1363, 1365, 1366, 1367, 1369, 1407, 1415, 1419, 1427, 1440, 1456, 1460, 1470, 1472, 1491, 1530]\n",
      "\n",
      "CAN IDs only in a specific dataset (2): [2016, 2024]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nCAN IDs in every dataset ({len(ids_inter)}): {sorted(list(ids_inter))}')\n",
    "print(f'\\nCAN IDs only in a specific dataset ({len(ids_union - ids_inter)}): {sorted(list(ids_union - ids_inter))}')\n",
    "target_ids = sorted(list(ids_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:19:36.501858Z",
     "start_time": "2024-05-09T01:19:36.031264Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>5.005908767101802</td>\n",
       "      <td>0.07777456938556762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>20.01950486900123</td>\n",
       "      <td>0.14149859670661089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>50.01968299595969</td>\n",
       "      <td>0.16222209139913107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>20.008175651764763</td>\n",
       "      <td>0.10171775778745148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>20.008075550081486</td>\n",
       "      <td>0.10489129536022514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>50.14235480807693</td>\n",
       "      <td>0.34947101707558975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>50.020279124878</td>\n",
       "      <td>0.16837370854916398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>50.02018291127854</td>\n",
       "      <td>0.16734612501747362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.0007424859257806266</td>\n",
       "      <td>0.027238478747113392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.001590976553594607</td>\n",
       "      <td>0.054764255173319586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean                  std\n",
       "1040     5.005908767101802  0.07777456938556762\n",
       "1078     20.01950486900123  0.14149859670661089\n",
       "1151     50.01968299595969  0.16222209139913107\n",
       "1168    20.008175651764763  0.10171775778745148\n",
       "1170    20.008075550081486  0.10489129536022514\n",
       "...                    ...                  ...\n",
       "899      50.14235480807693  0.34947101707558975\n",
       "902        50.020279124878  0.16837370854916398\n",
       "903      50.02018291127854  0.16734612501747362\n",
       "2016 0.0007424859257806266 0.027238478747113392\n",
       "2024  0.001590976553594607 0.054764255173319586\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.9954650628489626</td>\n",
       "      <td>0.044292683556723965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.9976073837876621</td>\n",
       "      <td>0.0002634018710625745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.9490001651426253</td>\n",
       "      <td>0.1966017799689266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.9995112136592371</td>\n",
       "      <td>0.00012761940997246292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.009999059567974351</td>\n",
       "      <td>0.0003118041254577751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>0.999510740474479</td>\n",
       "      <td>0.0007597910838317145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>0.9996947733218331</td>\n",
       "      <td>0.0008563397188535942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>1.0009190113701798</td>\n",
       "      <td>0.0010015640158069577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>11.029508</td>\n",
       "      <td>9.050514872829723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>5.511082999999999</td>\n",
       "      <td>9.164676961945112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean                    std\n",
       "66     0.9954650628489626   0.044292683556723965\n",
       "67     0.9976073837876621  0.0002634018710625745\n",
       "68     0.9490001651426253     0.1966017799689266\n",
       "127    0.9995112136592371 0.00012761940997246292\n",
       "128  0.009999059567974351  0.0003118041254577751\n",
       "...                   ...                    ...\n",
       "1472    0.999510740474479  0.0007597910838317145\n",
       "1491   0.9996947733218331  0.0008563397188535942\n",
       "1530   1.0009190113701798  0.0010015640158069577\n",
       "2016            11.029508      9.050514872829723\n",
       "2024    5.511082999999999      9.164676961945112\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The mean and standard deviation of time intervals (ID-specific)\n",
    "ignore_cols = ['Session', 'Time']\n",
    "df_cnt_gauss = pd.concat(\n",
    "    [df_cnt_concat.drop(columns=ignore_cols).mean().rename('mean'), \n",
    "     df_cnt_concat.drop(columns=ignore_cols).std().rename('std')], \n",
    "    axis=1\n",
    ")\n",
    "df_itv_gauss = pd.concat(\n",
    "    [df_itv_concat.drop(columns=ignore_cols).mean().rename('mean'), \n",
    "     df_itv_concat.drop(columns=ignore_cols).std().rename('std')], \n",
    "    axis=1\n",
    ")\n",
    "display(df_cnt_gauss)\n",
    "display(df_itv_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_cnt_concat, df_itv_concat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Normalized Likelihood Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:20:18.593066Z",
     "start_time": "2024-05-09T01:20:18.580665Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if gap <= mean, use interval values, and if gap > mean, use gap values\n",
    "def interval_with_gap_condition(data: pd.DataFrame, ids: list): # gauss: pd.DataFrame):\n",
    "    data = data[['Session', 'SessionCat', 'Time', 'ID']].copy()\n",
    "    n_rows = data.shape[0]\n",
    "    t_cut = time_cut(data)\n",
    "    data['Interval'] = data.groupby('ID')['Time'].diff()\n",
    "    data = pd.concat([data[['Session', 'SessionCat', 'Time']], data.pivot(columns='ID', values=['Time', 'Interval'])], axis=1)\n",
    "    ft_dict = {\n",
    "        'Session': data['Session'].to_list(),\n",
    "        'SessionCat': data['SessionCat'].to_list(),\n",
    "        'Time': data['Time'].to_list()\n",
    "    }\n",
    "    for id in ids:\n",
    "        data[('Gap', id)] = data['Time'] - data[('Time', id)].ffill()\n",
    "        data[('Interval', id)] = data[('Interval', id)].ffill()\n",
    "        data[('Gap', id)] = data[[('Interval', id), ('Gap', id)]].max(axis=1)\n",
    "        ft_dict[id] = data[('Gap', id)]\n",
    "    ft_df = pd.DataFrame.from_dict(ft_dict)\n",
    "    ft_df = ft_df.loc[ft_df['Time'] >= t_cut]\n",
    "    return ft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:20:28.611345Z",
     "start_time": "2024-05-09T01:20:28.573600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get normal distribution classes\n",
    "norm_dist = {'cnt': {}, 'itv': {}}\n",
    "for id in target_ids:\n",
    "    cnt_mean = df_cnt_gauss.loc[id, 'mean']\n",
    "    cnt_std = df_cnt_gauss.loc[id, 'std']\n",
    "    itv_mean = df_itv_gauss.loc[id, 'mean']\n",
    "    itv_std = df_itv_gauss.loc[id, 'std']\n",
    "    if cnt_std == 0:    # Some IDs have constant counts at all times, which means std = 0\n",
    "        print('cnt', id, cnt_mean, cnt_std)\n",
    "        norm_dist['cnt'][id] = norm(cnt_mean, cnt_mean * 0.001)   # Set std to a sufficiently small value\n",
    "    else:\n",
    "        norm_dist['cnt'][id] = norm(cnt_mean, cnt_std)\n",
    "    if itv_std == 0:    # Some IDs have constant counts at all times, which means std = 0\n",
    "        print('itv', id, itv_mean, itv_std)\n",
    "        norm_dist['itv'][id] = norm(itv_mean, itv_mean * 0.001)   # Set std to a sufficiently small value\n",
    "    else:\n",
    "        norm_dist['itv'][id] = norm(itv_mean, itv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:20:55.208584Z",
     "start_time": "2024-05-09T01:20:53.279541Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f761c351f34f9eaff2cba563ef4c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been truncated to remove NaN (5,960 rows removed)\n",
      "Data has been truncated to remove NaN (7,268 rows removed)\n",
      "Data has been truncated to remove NaN (58,764 rows removed)\n",
      "Data has been truncated to remove NaN (6,805 rows removed)\n",
      "Max range (row-wise): {'cnt': [-4286.379269796721, 72.16759954834816], 'itv': [-221136.8489468792, 392.66707133555195]}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Get the maximum value of likelihoods (of the training set)\n",
    "df_cnt_likelihoods, df_itv_likelihoods = list(), list()\n",
    "for data_file in tqdm(data_files['train']):\n",
    "    df = load_arrange_data(data_file, dataset=DATASET, print_option=False)\n",
    "    df_itv = interval_with_gap_condition(df, ids=target_ids) # gauss=df_gauss)\n",
    "    df_cnt = get_counts(df, cut=True)\n",
    "    cnt_likelihood, itv_likelihood = dict(), dict()\n",
    "    for id in target_ids:\n",
    "        cnt_likelihood[id] = norm_dist['cnt'][id].logpdf(df_cnt[id])\n",
    "        itv_likelihood[id] = norm_dist['itv'][id].logpdf(df_itv[id])\n",
    "    df_cnt_likelihoods.append(pd.DataFrame.from_dict(cnt_likelihood))\n",
    "    df_itv_likelihoods.append(pd.DataFrame.from_dict(itv_likelihood))\n",
    "\n",
    "max_range = {'cnt': [0, 0], 'itv': [0, 0]}  # [min, max]\n",
    "for df in df_cnt_likelihoods:    \n",
    "    current_max = df.sum(axis=1).max()\n",
    "    current_min = df.sum(axis=1).min()\n",
    "    if max_range['cnt'][0] > current_min:\n",
    "        max_range['cnt'][0] = current_min\n",
    "    if max_range['cnt'][1] < current_max:\n",
    "        max_range['cnt'][1] = current_max\n",
    "for df in df_itv_likelihoods: \n",
    "    current_max = df.sum(axis=1).max()\n",
    "    current_min = df.sum(axis=1).min()\n",
    "    if max_range['itv'][0] > current_min:\n",
    "        max_range['itv'][0] = current_min\n",
    "    if max_range['itv'][1] < current_max:\n",
    "        max_range['itv'][1] = current_max\n",
    "print(f'Max range (row-wise): {max_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_cnt_likelihoods, df_itv_likelihoods\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Update Session Labels and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:35:18.610253Z",
     "start_time": "2024-05-09T01:35:18.602419Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Update session label\n",
    "# - if an attack message is used to extract feature, label as attack\n",
    "# - which means stop labeling as attacks if every ID receives two normal messages\n",
    "def update_label(data: pd.DataFrame, dataset: str, ids: list):\n",
    "    if dataset == 'X-CANIDS':\n",
    "        data['LabelBin'] = 0\n",
    "        data.loc[data['Label'] != 'Normal', 'LabelBin'] = 1\n",
    "        data['Lag2Label'] = data.groupby('ID')['LabelBin'].shift(2)\n",
    "    elif dataset == 'Syncan':\n",
    "        data['Lag2Label'] = data.groupby('ID')['Session'].shift(2)\n",
    "    data['Lag2Label'] = data['Lag2Label'].fillna(0)\n",
    "    attack_points = data.loc[data['Session'] != data['Session'].shift(1).fillna(0), 'Time'].to_list()\n",
    "    attack_names = data.loc[data['Session'] != data['Session'].shift(-1).fillna(0), 'SessionCat'].to_list()\n",
    "    assert len(attack_points) % 2 == 0, attack_points\n",
    "    attack_ranges = []\n",
    "    for i in range(len(attack_points) // 2):\n",
    "        attack_ranges.append(attack_points[2*i:2*(i+1)])\n",
    "    for i in range(len(attack_ranges)):\n",
    "        end = attack_ranges[i][1]\n",
    "        attack = attack_names[2 * i + 1]\n",
    "        if i < len(attack_ranges) - 1:\n",
    "            next_start = attack_ranges[i+1][0]\n",
    "        else:   # in case of the last attack\n",
    "            next_start = end + 10\n",
    "        query = (end < data['Time']) & (data['Time'] < next_start) & (data['Lag2Label'] == 1) & (data['ID'].isin(ids))\n",
    "        effect_end = data.loc[query, 'Time'].max()\n",
    "        if not np.isnan(effect_end):\n",
    "            print(f'({i+1}) {attack} affects until {effect_end}s, {effect_end - end:.2f}s passed after the attack.')\n",
    "            data.loc[(data['Time'] >= end) & (data['Time'] < effect_end), 'Session'] = 1\n",
    "            data.loc[(data['Time'] >= end) & (data['Time'] < effect_end), 'SessionCat'] = attack\n",
    "    return data\n",
    "\n",
    "def update_label_window(data: pd.DataFrame, dataset: str, ids: list, window=1):\n",
    "    attack_points = data.loc[data['Session'] != data['Session'].shift(1).fillna(0), 'Time'].to_list()\n",
    "    attack_names = data.loc[data['Session'] != data['Session'].shift(-1).fillna(0), 'SessionCat'].to_list()\n",
    "    assert len(attack_points) % 2 == 0, attack_points\n",
    "    attack_ranges = []\n",
    "    for i in range(len(attack_points) // 2):\n",
    "        attack_ranges.append(attack_points[2*i:2*(i+1)])\n",
    "    for i in range(len(attack_ranges)):\n",
    "        end = attack_ranges[i][1]\n",
    "        attack = attack_names[2 * i + 1]\n",
    "        print(f'({i+1}) {attack} affects until {end + window} s, {window} s passed after the attack.')\n",
    "        data.loc[(data['Time'] >= end) & (data['Time'] < end + window), 'Session'] = 1\n",
    "        data.loc[(data['Time'] >= end) & (data['Time'] < end + window), 'SessionCat'] = attack\n",
    "    return data\n",
    "\n",
    "def z_extraction(data_cnt: pd.DataFrame, data_itv: pd.DataFrame, ids: list, norm_dict: dict, likelihood_range: dict):\n",
    "    assert len(data_cnt) == len(data_itv)\n",
    "    l_cnt_dict, l_itv_dict = dict(), dict()\n",
    "    for id in ids:\n",
    "        l_cnt_dict[id] = norm_dict['cnt'][id].logpdf(data_cnt[id])\n",
    "        l_itv_dict[id] = norm_dict['itv'][id].logpdf(data_itv[id])\n",
    "    l_cnt_df = pd.DataFrame.from_dict(l_cnt_dict)\n",
    "    l_itv_df = pd.DataFrame.from_dict(l_itv_dict)\n",
    "    l_df = data_cnt[['Session', 'SessionCat', 'Time']].copy()\n",
    "    l_df['z_cnt'] = ((l_cnt_df.sum(axis=1) - likelihood_range['cnt'][0]) / (likelihood_range['cnt'][1] - likelihood_range['cnt'][0])).values\n",
    "    l_df['z_itv'] = ((l_itv_df.sum(axis=1) - likelihood_range['itv'][0]) / (likelihood_range['itv'][1] - likelihood_range['itv'][0])).values\n",
    "    return l_df\n",
    "\n",
    "def get_save_path(original_path: str, dataset: str):\n",
    "    if dataset == 'X-CANIDS':\n",
    "        new_path = f'{Path(original_path).parents[1]}/z/{Path(original_path).stem}.parquet'\n",
    "    elif dataset == 'Syncan':\n",
    "        new_path = f'{Path(original_path).parent}/z_{Path(original_path).stem.split(\"_\")[-1]}.parquet'\n",
    "    return new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:55:37.941980Z",
     "start_time": "2024-05-09T01:52:30.837917Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b808231a2d4b4d8785197eb5a7ce92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session\n",
      "0    4279909\n",
      "Name: count, dtype: int64\n",
      "Data has been truncated to remove NaN (50,475 rows removed)\n",
      "Session\n",
      "0    4229434\n",
      "Name: count, dtype: int64\n",
      "../../Dataset/X-CANIDS/z/dump6.parquet is saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for step, file_list in data_files.items():\n",
    "    for data_file in tqdm(file_list, desc=f'Processing {step} datasets'):\n",
    "        # Update session labels and extract z features\n",
    "        df_cnt = load_arrange_data(data_file, dataset=DATASET, print_option=False)\n",
    "        print(df_cnt.Session.value_counts())\n",
    "        df_itv = df_cnt.copy()\n",
    "\n",
    "        start_time = time.process_time()\n",
    "        \n",
    "        df_itv = interval_with_gap_condition(df_itv, ids=target_ids) # gauss=df_gauss)\n",
    "        df_cnt = get_counts(df_cnt, ids=target_ids, cut=True, sessioncat=True)\n",
    "        df_z = z_extraction(df_cnt, df_itv, ids=target_ids, norm_dict=norm_dist, likelihood_range=max_range)\n",
    "\n",
    "        end_time = time.process_time()\n",
    "        \n",
    "        assert df_z.query('Session == 1 and SessionCat == \"Normal\"').shape[0] == 0\n",
    "        assert df_z.query('Session == 0 and SessionCat != \"Normal\"').shape[0] == 0\n",
    "\n",
    "        # Save as a parquet file\n",
    "        str_cols = [str(c) for c in df_z.columns]\n",
    "        df_z.columns = str_cols\n",
    "        save_path = get_save_path(data_file, dataset=DATASET)\n",
    "        df_z.to_parquet(save_path)\n",
    "        print(df_z.Session.value_counts())\n",
    "        print(f'{save_path} is saved.')\n",
    "        \n",
    "# Measure inference speed\n",
    "process_time = end_time - start_time\n",
    "inference_speed = len(df_itv) / process_time\n",
    "print(\"-----------------------------------------\")\n",
    "print(f'CPU execution time: {process_time:,} seconds')\n",
    "print(f'Inference speed: {inference_speed:.2f} messages per second')\n",
    "print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T01:52:24.458993Z",
     "start_time": "2024-05-09T01:52:24.349258Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_cnt, df_itv, df_z\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "### Move(copy) z data files to the result folder"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T06:50:51.760369Z",
     "start_time": "2024-09-13T06:50:51.757876Z"
    }
   },
   "source": [
    "######### SELECT THE DATASET #########\n",
    "# DATASET = 'Syncan'\n",
    "DATASET = 'X-CANIDS'\n",
    "DATASET_DIR = f'../../Dataset/{DATASET}'\n",
    "\n",
    "if DATASET == 'X-CANIDS':\n",
    "    feature_files = {\n",
    "        'train': glob.glob(f'{DATASET_DIR}/z/dump[1-4].parquet'),\n",
    "        'valid': [f'{DATASET_DIR}/z/dump5.parquet'],\n",
    "        'test': glob.glob(f'{DATASET_DIR}/z/dump6-*.parquet'),\n",
    "    }\n",
    "elif DATASET == 'Syncan':\n",
    "    feature_files = {\n",
    "        'train': [\n",
    "            f'{DATASET_DIR}/z_train.parquet'\n",
    "        ],\n",
    "        'valid': [\n",
    "            f'{DATASET_DIR}/z_valid.parquet'\n",
    "        ],\n",
    "        'test': [\n",
    "            f'{DATASET_DIR}/z_normal.parquet',\n",
    "            f'{DATASET_DIR}/z_flooding.parquet',\n",
    "            f'{DATASET_DIR}/z_plateau.parquet',\n",
    "            f'{DATASET_DIR}/z_continuous.parquet',\n",
    "            f'{DATASET_DIR}/z_playback.parquet',\n",
    "            f'{DATASET_DIR}/z_suppress.parquet'\n",
    "        ]\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:56:28.367499600Z",
     "start_time": "2023-06-26T08:55:59.901787100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fda59d2794849dfad119199c15b41e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_normal.parquet saved.\n",
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_flooding.parquet saved.\n",
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_plateau.parquet saved.\n",
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_continuous.parquet saved.\n",
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_playback.parquet saved.\n",
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_suppress.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "# test results\n",
    "dt_exp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "for ft_file in tqdm(feature_files['test']):\n",
    "    df = pd.read_parquet(ft_file)\n",
    "    results = df\n",
    "    if DATASET == 'Syncan':\n",
    "        attack = Path(ft_file).stem.split('_')[-1]\n",
    "        save_path = f'../../Results/{DATASET}_TIL_{dt_exp}_{attack}.parquet'\n",
    "        results.to_parquet(save_path)\n",
    "    elif DATASET == 'X-CANIDS':\n",
    "        attack = '-'.join(Path(ft_file).stem.split('-')[1:])\n",
    "        save_path = f'../../Results/{DATASET}_TIL_{dt_exp}_{attack}.parquet'\n",
    "        results.to_parquet(save_path)\n",
    "    print(f'{save_path} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:56:57.221687500Z",
     "start_time": "2023-06-26T08:56:57.205415500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 55.41 KB\n"
     ]
    }
   ],
   "source": [
    "# memory usage\n",
    "objs = [df_itv_gauss, df_cnt_gauss, max_range, max_range]\n",
    "print(f'Model size: {asizeof.asizeof(objs)/1024:.2f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T06:53:20.373936600Z",
     "start_time": "2023-06-23T06:53:17.328330500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Results/Syncan_TIL_2024-07-19_09-29-18_valid.parquet saved.\n"
     ]
    }
   ],
   "source": [
    "# validation results (for setting threshold)\n",
    "ft_file = feature_files['valid'][0]\n",
    "df = pd.read_parquet(ft_file)\n",
    "results = df\n",
    "if DATASET == 'Syncan':\n",
    "    attack = Path(ft_file).stem.split('_')[-1]\n",
    "    save_path = f'../../Results/{DATASET}_TIL_{dt_exp}_valid.parquet'\n",
    "    results.to_parquet(save_path)\n",
    "elif DATASET == 'X-CANIDS':\n",
    "    attack = '-'.join(Path(ft_file).stem.split('-')[1:])\n",
    "    save_path = f'../../Results/{DATASET}_TIL_{dt_exp}_valid.parquet'\n",
    "    results.to_parquet(save_path)\n",
    "print(f'{save_path} saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
